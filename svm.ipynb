{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2a8617-c876-4ac6-9470-2f58944c1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import skimage\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecdedd8-3e98-43f5-b9a9-e9af7a0fb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(1337)\n",
    "samples_path = Path(\"data/processed/crop_6_1000x1000/\")\n",
    "data_dir = Path(\"data/sets/6_999x999/\")\n",
    "img_width = img_height = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "912643dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(img_np):\n",
    "    \"\"\"Normalize CV2-image which default is uint8\"\"\"\n",
    "    return cv2.normalize(src=img_np, dst=None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "def load_img(img_path, color_mode, shape, img_filter):\n",
    "    color = cv2.IMREAD_COLOR if color_mode == \"rgb\" else cv2.IMREAD_GRAYSCALE\n",
    "    img = normalize_img(cv2.resize(cv2.imread(str(img_path), color), dsize=shape))\n",
    "    if img_filter is not None:\n",
    "        img = img_filter(img)\n",
    "    if color_mode == \"rgb\": return img[:, :, ::-1] # Converting to RGB from BGR\n",
    "    return img\n",
    "\n",
    "def load_dataset(data_dir, num_samples=50, color=\"rgb\", shape=(128,128), sets=[\"train\"], categories=[\"dc\", \"marvel\"], img_filter=None):\n",
    "    target_arr, flat_data_arr = [], []\n",
    "    for set in sets:\n",
    "        print(\"Loading data set:\", set)\n",
    "        set_path = data_dir / set \n",
    "        for idx, category in enumerate(categories):\n",
    "            print(f\"Loading category : {category}\")\n",
    "            dir_path = set_path / category\n",
    "            all_files = list(dir_path.iterdir())\n",
    "            print(\"Number of files:\", len(all_files))\n",
    "            if num_samples is None or num_samples > len(all_files):\n",
    "                cat_samples = len(all_files)\n",
    "                \n",
    "            files = rng.permutation(all_files)[:cat_samples]\n",
    "            print(\"Number of files to load:\", len(files))\n",
    "\n",
    "            category_imgs = [load_img(img_path, color, shape, img_filter).flatten() for img_path in files]\n",
    "            flat_data_arr.extend(category_imgs)\n",
    "            target_arr.extend([idx] * len(category_imgs))\n",
    "        print(\"---------------------------\")\n",
    "\n",
    "    X_flattened = np.array(flat_data_arr)\n",
    "    y = np.array(target_arr)\n",
    "    return X_flattened, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e702994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effects of the different pre processing methods \n",
    "test_img = cv2.resize(cv2.imread(str(data_dir / \"train\" / \"marvel\" / \"marvel_134.png\"), cv2.IMREAD_GRAYSCALE), dsize=(512,512))\n",
    "normalized_img = normalize_img(test_img)\n",
    "normalized_hist_shifted = skimage.exposure.equalize_hist(test_img)\n",
    "norm_adapt_hist = skimage.exposure.equalize_adapthist(test_img)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "test_imgs = [test_img, normalized_img, normalized_hist_shifted, norm_adapt_hist]\n",
    "row, col = 2, 2\n",
    "for i in range(1,5):\n",
    "    fig.add_subplot(row, col, i)\n",
    "    plt.imshow(test_imgs[i-1], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f739441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training samples\n",
    "samples_train, labels_train = load_dataset(data_dir, num_samples=None, shape=(img_width, img_height), color=\"gray\", img_filter=None)\n",
    "# Randomize samples in case of picking a subset\n",
    "random_idx_train = rng.choice(np.arange(labels_train.size), labels_train.size, replace=False)\n",
    "X_train, y_train = samples_train[random_idx_train], labels_train[random_idx_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d63fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test samples\n",
    "samples_test, labels_test = load_dataset(data_dir, num_samples=None, sets=[\"test\"], shape=(img_width, img_height), img_filter=None)\n",
    "random_idx_test = rng.choice(np.arange(labels_test.size), labels_test.size, replace=False)\n",
    "X_test, y_ttest = samples_test[random_idx_test], labels_test[random_idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c0c2363-64fc-44e2-9f99-03c626998eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyper parameters \n",
    "param_grid = {\"C\": [0.1, 1, 10, 100], \"degree\":[3], \"gamma\": [0.0001, 0.001, 0.01, 0.1, 1, 10,], \"kernel\": [\"rbf, poly\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "715a439e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3716, 49152), (3716,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e1fe9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_set_sizes = [10, 25, 50, 100, 250, 500, 1000, 4000]\n",
    "def parameter_evaluation(samples, labels, set_sizes=default_set_sizes):\n",
    "    \"\"\"Method to run the GridSearchCV on various sample sizes of each category\"\"\"\n",
    "    accuracies = []\n",
    "    for sample_set in set_sizes:\n",
    "        print(\"Test:\", sample_set)\n",
    "        idx = np.random.choice(np.arange(labels.size), sample_set, replace=False)\n",
    "        X_train, y = samples[idx], labels[idx]\n",
    "        print(\"Label split:\", np.bincount(y))\n",
    "\n",
    "\n",
    "        svc = svm.SVC(probability=False)\n",
    "        param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [0.0001, 0.001, 0.01, 0.1, 1, 10], \"kernel\":[\"rbf\"]}\n",
    "        model = GridSearchCV(svc, param_grid, verbose=3, n_jobs=-1, error_score=\"raise\")\n",
    "        model.fit(X_train, y)\n",
    "        score = model.best_score_\n",
    "        \n",
    "        print(\"Best score:\", score)\n",
    "        print(\"Best params\", model.best_params_)\n",
    "        accuracies.append(score)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = parameter_evaluation(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ace93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best achieved accuracies as a function of the training data size\n",
    "plt.plot(np.array(default_set_sizes), accuracies)\n",
    "plt.title(\"Test accuracies by training data set size\")\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.savefig(\"svm_cross-valid_rbf_accuracy_by_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a444bb",
   "metadata": {},
   "source": [
    "## Kernel SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66753f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes a saved PCA model based on 256x256 images \n",
    "PCA = load(\"pca_model_256x256_grayscale_2086_components.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the sampels according to the PCA generated components\n",
    "samples_train_PCA, labels_train_pca = PCA.transform(samples_train), labels_train\n",
    "random_idx_pca_train = rng.choice(np.arange(labels_train_pca.size), labels_train_pca.size, replace=False)\n",
    "X_train_pca, y_train_pca = samples_train[random_idx_pca_train], labels_train[random_idx_pca_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_PCA = parameter_evaluation(X_train_pca, y_train_pca) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(default_set_sizes), accuracies)\n",
    "plt.title(\"Test accuracies by training data set size\")\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.savefig(\"svm_cross-valid_rbf_accuracy_by_size\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
